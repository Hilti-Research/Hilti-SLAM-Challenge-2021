# The Hilti SLAM Challenge

<p align="center">
  <img src="https://github.com/hemi86/hiltislamchallenge/blob/be56ed92d5415dff49e53b58bbeab019f923f883/images/HILTI_SLAM_CHALLENGE_VISUAL.jpg"  width="70%"  alt="HILTI SLAM Challeng"/>
</p>


This is the Github repository for the SLAM challenge hosted at https://www.hilti-challenge.com/. The main purpose of the repository is to allow easy discussion and the reporting of issues.

## Description
The participants are required to run their SLAM algorithms on sequences from the HILTI-Challenge Dataset, which includes images, IMU measurements, and LIDAR data recorded with a custom handheld device. The goal is to estimate the position of the handheld device as accurately as possible, utilizing any desired sensor combinations. The winner will be selected based on the accuracy of the estimated trajectories and will be awarded 7,000 USD and will also be invited to present their approach at the IROS 2021 Workshop [Perception and Navigation for Autonomous Robotics in Unstructured and Dynamic Environments](https://iros2021-pnarude.github.io/) taking place on September 27th, 2021 in Prague. The second and third ranked teams will be awared  2,000 USD and 1,000 USD repectively. Only participants affiliated with educational institutions (Students, Postdocs) are eligible to win the cash prize.

## Groundtruth
We do not provide ground truth for all datasets. Please you our evaluation system [here](https://submit.hilti-challenge.com/).

## Winners

Please visit https://www.hilti-challenge.com/ for the leaderboard.

## Datasets

The datasets can be downloaded [here](https://www.hilti-challenge.com/dataset.html)

## Publication

When using this work in an academic context, please cite the following publication:

~~~
@misc{2109.11316,
Author = {Michael Helmberger and Kristian Morin and Nitish Kumar and Danwei Wang and Yufeng Yue and Giovanni Cioffi and Davide Scaramuzza},
Title = {The Hilti SLAM Challenge Dataset},
Year = {2021},
Eprint = {arXiv:2109.11316},
}
~~~

## Acknowledgement

We would like to thank to thank Danwei Wang, Christian Laugier, Philippe Martinet, Yufeng Yue for hosting our challenge at the IROS2021 workshop “Perception and Navigation for Autonomous Robotics in Unstructured and Dynamic Environments”. Futher, we thank Prof. Davide Scaramuzza and Giovanni Coffi for the great support in organizing the challenge, verifying the data and providing the [UZH FPV Challenge](https://fpv.ifi.uzh.ch/) as template for this challenge.


## License

All datasets and benchmarks on this page are copyright by us and published under the [Creative Commons Attribution-NonCommercial-ShareAlike 3.0](https://creativecommons.org/licenses/by-nc-sa/3.0/) License. This means that you must attribute the work in the manner specified by the authors, you may not use this work for commercial purposes and if you alter, transform, or build upon this work, you may distribute the resulting work only under the same license.


